{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_weights = vgg16.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(vgg16_weights), len(vgg16_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(vgg16.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in vgg16_weights:\n",
    "    print(len(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOC2012 Segmentation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = io.imread('VOC2012/SegmentationClass/2007_000032.png')\n",
    "print(type(img), img.shape)\n",
    "io.imshow(img)\n",
    "io.show()\n",
    "\n",
    "mask = np.ndarray((281, 500))\n",
    "\n",
    "for i in range(len(img)):\n",
    "    for j in range(len(img[0])):\n",
    "        if img[i][j].any() != 0:\n",
    "            mask[i][j] = 1\n",
    "        else:\n",
    "            mask[i][j] = 0\n",
    "\n",
    "io.imshow(mask)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models.fcn32 import FCN32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn32 = FCN32()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn32_weights = fcn32.get_weights()\n",
    "print(len(fcn32_weights))\n",
    "\n",
    "for layer in fcn32_weights:\n",
    "    print(len(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_weights(fcn, vgg):\n",
    "    for i in range(18):\n",
    "        fcn.layers[i].set_weights(vgg.layers[i+1].get_weights())\n",
    "\n",
    "set_weights(fcn32, vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.loss import cross_entropy_logits\n",
    "from utils.metric import logits_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logits_loss(y_true, y_pred):\n",
    "    return sum([y_true[i][j]^y_pred[i][j] for i in range(224) for j in range(224)])\n",
    "\n",
    "def logits_accuracy(y_true, y_pred):\n",
    "    return 1 - logits_loss(y_true, y_pred)/(224.0*224.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "fcn32.compile(loss=logits_loss,\n",
    "              optimizer=adam,\n",
    "              metrics=[logits_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from keras import backend as K\n",
    "\n",
    "img = io.imread('VOC2012/JPEGImages/2007_000032.jpg')\n",
    "#io.imshow(img)\n",
    "#io.show()\n",
    "img =resize(img, (224, 224, 3))\n",
    "img = np.expand_dims(img, 0)\n",
    "pred = fcn32.predict(img, batch_size=1)\n",
    "print(type(pred), pred.shape)\n",
    "\n",
    "pred = np.argmax(pred, axis=3)\n",
    "pred = np.squeeze(pred)\n",
    "print(type(pred), pred.shape)\n",
    "\n",
    "io.imshow(pred)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOC Segmentation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import skimage.io as io\n",
    "import os.path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_to_img(seg, img_path):\n",
    "    # convert an image file path into a corresponding mask file path \n",
    "    dirname, basename = os.path.split(seg)\n",
    "    imgname = basename.replace(\".png\", \".jpg\")\n",
    "    return os.path.join(img_path, imgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segs = [seg for seg in glob.glob(\"VOC2012/SegmentationClass/*\")]\n",
    "pairs = [(seg_to_img(seg, \"VOC2012/JPEGImages/\"), seg) for seg in segs]\n",
    "print(\"number of image segmentation pairs: \", len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seg_to_mask(img):\n",
    "    mask = np.ndarray((224, 224))\n",
    "    for i in range(len(img)):\n",
    "        for j in range(len(img[0])):\n",
    "            if img[i][j].any() != 0:\n",
    "                mask[i][j] = 1\n",
    "            else:\n",
    "                mask[i][j] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    seg_names = [seg for seg in glob.glob(\"VOC2012/SegmentationClass/*\")]\n",
    "    img_names = [seg_to_img(seg, \"VOC2012/JPEGImages/\") for seg in seg_names]\n",
    "    imgs = np.ndarray((2913, 224, 224, 3))\n",
    "    masks = np.ndarray((2913, 224, 224))\n",
    "    for i in range(2913):\n",
    "        seg =resize(io.imread(seg_names[i]), (224, 224, 3))\n",
    "        masks[i] = seg_to_mask(seg)\n",
    "        img = resize(io.imread(img_names[i]), (224, 224, 3))\n",
    "        imgs[i] = img\n",
    "    return imgs, masks\n",
    "\n",
    "imgs, masks = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"imgs.npy\", imgs)\n",
    "np.save(\"masks.npy\", masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(imgs), imgs.shape)\n",
    "print(type(masks), masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_masks = np.ndarray((2913, 224, 224, 1))\n",
    "\n",
    "for i in range(2913):\n",
    "    new_masks[i] = np.expand_dims(masks[i], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(new_masks), new_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FCN32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run on gpu0\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcn32_history = fcn32.fit(imgs, new_masks, \n",
    "          batch_size=32, \n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model and weights\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'fcn32.h5'\n",
    "\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "fcn32.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = fcn32.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(fcn32_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN32 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from keras import backend as K\n",
    "\n",
    "img = io.imread('VOC2012/JPEGImages/2007_000123.jpg')\n",
    "img =resize(img, (224, 224, 3))\n",
    "io.imshow(img)\n",
    "io.show()\n",
    "img = np.expand_dims(img, 0)\n",
    "pred = fcn32.predict(img, batch_size=1)\n",
    "#print(type(pred), pred.shape)\n",
    "\n",
    "pred = np.argmax(pred, axis=3)\n",
    "pred = np.squeeze(pred)\n",
    "#print(type(pred), pred.shape)\n",
    "\n",
    "io.imshow(pred)\n",
    "io.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
